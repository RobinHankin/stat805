---
output:
  pdf_document: default
  html_document: default
---
#  Univariate differentiation {#calculus}

\newcommand{\prob}[1]{\operatorname{\mathrm{Prob}}\left({#1}\right)}
\newcommand{\like}[1]{\operatorname{\mathcal{L}}\left(#1\right)}
\newcommand{\supp}[1]{\operatorname{\mathcal{S}}\left(#1\right)}

\newcommand{\likeval}{\mathcal{L}}
\newcommand{\suppval}{\mathcal{S}}


## Introduction

(There are many many online resources for calculus at every level of
sophistication, ranging from secondary school material to exterior
calculus on manifolds.  This short manual gives an introduction to the
basics of calculus.  It is informal, non-rigorous and focusses on
practical applications.  If you want a rigorous introduction to
calculus, see either of the two books by Spivak in the references).

You will recall the basics of univariate calculus from the figure:

\includegraphics{calculus1}

The idea is to give the *slope* of the curve as $\frac{dy}{dx}$, or
sometimes written $f'(x)$.  We usually write

\[
\delta y=f'(x)\delta x\]

so we have

\[
f(x+\delta x) = f(x) + \delta x\times f(x)\qquad\mbox{to first order}
\]

You will recall that differentiation is an
algebraic process and some examples follow:


* $f(x)=x^3\longrightarrow f'(x)=3x^2$
* $f(x)=x^7\longrightarrow f'(x)=7x^6$
* $f(x)=x^n\longrightarrow f'(x)=nx^{n-1}$
* $f(x)=\sin(x)\longrightarrow f'(x)=\cos(x)$
* $f(x)=\exp(x)\longrightarrow f'(x)=\exp(x)$

Differentiation is linear which means that we can say things like

* $f(x)=3x^7\longrightarrow f'(x)=21x^6$
* $f(x)=x^3+x^6\longrightarrow f'(x)=3x^2+6x^5$
* $f(x)=x^3+\sin(x)\longrightarrow f'(x)=3x^2+\cos(x)$


There are also rules for iterated functions and products:

* $f(x) = x^2\sin(x)\longrightarrow f'(x)=x^2\cos(x) + 2x\sin(x)$
* $f(x) = \sin(x^2)\longrightarrow f'(x)=2x\cos(x^2)$.

See any calculus textbook for reminders.



## Stop: verification.

We will now verify that one of these formulae is correct using R.
Consider $f(x)=x^3+x^6$.  Taking $x=1.2$ as an example, we have

```{r}
f <- function(x){x^3 + x^6}
f(1.2)
1.2^3 + 1.2^6
```

So the two methods agree.  Now algebra shows us that $f'(x)=3x^2+6x^5$
and we can verify this using $\delta x=0.01$:

```{r}
fdash <- function(x){3*x^2 + 6*x^5}
x <- 1.2        # define numerical value of x
dx <- 0.01       # notional small value
f(x)              # remind ourselves what f(x) is
f(x+dx)             # move a small distance along the x axis
f(x) + dx*fdash(x)   # close agreement!
```

Which provides numerical evidence that we have the correct formula for
$f'(x)$.  We will try something a little more complicated now:


```{r}
f <- function(x){sin(x+x^2/3)+x^2/10}  # overwrite f()
x <- seq(from=-3,to=3,len=100)       # define a sequence of x-values 
plot(x,f(x))                       # plot the function
abline(0,0)                      # mark y=0 for reference
```

This is quite a complicated function and although we could
differentiate it by algebra, we will use R to define a derivative
numerically:

```{r}
dx <- 1e-6   # notional small value
fdash <- function(x){(f(x+dx)-f(x))/dx}  # direct implementation of f'
plot(x,fdash(x),main="f'(x)")           # plot the derivative
abline(0,0)                            # mark y=0 for reference
```

Actually it might be  better to plot both $f$ and $f'$ on the same axes:


```{r}
plot(x,f(x),type='l',main="f and fdash")
points(x,fdash(x),type='l',col='red')
legend("topleft",lty=1,col=c("black","red"),legend=c("f(x)","fdash(x)"))
abline(0,0)
```

Spend some time checking that your understanding of function and
derivative is correct using the above figure.

Note that both maxima and minima have $f'=0$ and to distinguish these
we can calculate the sign of the second derivative.  If $f'(x)=0,
f''(x)>0$ we have a minimum and if $f'(x)=0,f''(x)<0$ we have a
maximum.

## Root finding

It is a very frequent occurence that we wish to identify *roots*, that
is, places where the function crosses the $x$-axis; symbolically we
would write $x\colon f(x)=0$.  Looking at the function we can see a
root at about -2.5, one at about 0, and one at about 2.1.  We will
find the exact positions of these using R.  But before we start using
R, we can say that $x=0$ is an _exact_ root of $f(x)=0$.  Can you see
why?

To find a root of a function ```f```, we use the ```uniroot()```
function; remember to type ```?uniroot``` at the R prompt to get help
on this functionality.

```{r}
uniroot(f,interval=c(-3,-1))
```

We can see from the different lines of output from ```uniroot()```
that the root is at ```-2.28051``` at which point the function has a
value of ```1.057589e-6```, which is very small.  To find the other
root we use a different starting interval:

```{r}
uniroot(f,interval=c(2,2.5))
```

As one further piece of verification we will check that zero is indeed
a root, which we discovered algebraically above:

```{r}
uniroot(f,interval=c(-0.1,0.1))
```

### verification

Always always always verify your findings.  We will take the values
found by ```uniroot()``` and mark them on a plot, just to check that
they do in fact correspond to roots:

```{r}
plot(x,f(x),axes=FALSE,type='l')  # basic plot, no axes
axis(1,pos=0) # x-axis, at (vertical) position 0
axis(2,pos=0) # y-axis, at (vertical) position 0
points(c(-2.28051,0,2.114632),c(0,0,0),pch=4,cex=3,col='red')
```

In the diagram above, the red crosses are plotted at the positions
given by ```uniroot()``` and we can see that these lie at the points
where the curve crosses the x-axis, that is, at the roots of function
```f()```.

## Optimization

Quite often we wish to minimize a function (typically the function
represents a cost of some sort, and we wish to minimize this).  In
calculus we observe that at a minimum, $f'(x)=0$ but this turns out to
be difficult to use in practice.  The R function we will use is called
```optimize()``` and this again takes a function and an interval.
Looking again at the graph of the function we see that there is one
minimum at about $x=-1.1$ and one at about $x=2.5$.


```{r}
optimize(f,c(-2,0))
```

So we see that there is a minimum at $x=-1.079651$.  Just to check, we
can evaluate our (numerical) derivative at this point:

```{r}
fdash(-1.079651)
```

showing that the slope of the line is indeed quite small at this
point.  We can find the other (local!) minimum:

```{r}
optimize(f,c(2,3))
```

To find a _maximum_ we simply minimize the negative of ```f()```:

```{r}
g <- function(x){-f(x)} # g(x) = -f(x)
optimize(g,c(-1,2))     # take care with signs
```


### Verification

Always always always verify your findings.  We will take the values
found by ```optim()``` and mark them on a plot, just to check that
they do in fact correspond to (local) maxima:

```{r}
plot(x,f(x),axes=FALSE)  # basic plot, no axes
axis(1,pos=0)           # x-axis, at  position 0
axis(2,pos=0)          # y-axis, at position 0
points(
    c(-1.079,2.477,1.214),   # x-coordinates of extrema
    c(-0.521,-0.368,+1.139), # y-coordinates of extrema
    pch=4,col="red",cex=4)   # control the appearance
```

In the diagram above, the red crosses are plotted at the positions
given by ```optim()``` and we can see that these lie at the points
where the curve has a (local) maximum or minimum, that is, at extrema
of ```f()```.


## Worked example: algebraic maximization.

We will maximize $1/x^2 - 1/x^3$, a function that occurs in
relativistic dynamics.  We first write $f(x)=x^{-2} - x^{-3}$ and
differentiate to obtain $f'(x) = -2x^{-3} + 3x^{-4}$.  Then we need to
solve $f'(x)=0$ for $x$ which amounts finding $x$ with $-2x^{-3} +
3x^{-4}=0$.  Algebra shows us that $x=3/2$ is the solution.

However, this point might be a maximum or a minimum (or indeed a point
of inflection) so we need to check that the *second* derivative is
negative at $x=3/2$.  We can say $f''(x) = 6x^{-4}-12x^{-5}$ and so
$f''(3/2) =
6\left(\frac{3}{2}\right)^{-4}-12\left(\frac{3}{2}\right)^{-5}\simeq
-0.395<0$ as required.

Always always always always always always always always always verify
your work numerically:


```{r}
x <- seq(from=0.78,to=6,len=100)
plot(x,1/x^2-1/x^3,type='b')
abline(h=0)
points(3/2,(2/3)^2-(2/3)^3,pch=16,cex=3,col='red')
```

In the above, the red dot correctly shows the position of the maximum.
Note that, when preparing this resource, I got the position wrong a
total of seven times before finding and correcting my errors
(including several R errors, several algebraic errors, and one
conceptual error) to give you the version you see here.  Checking is
difficult!

